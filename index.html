<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, user-scalable=no"
    />
    <title>Assistive AI</title>
    <style>
      /* ... existing style ... */
      #listenButton:disabled {
        background-color: #4b5563;
        opacity: 0.5;
        cursor: not-allowed;
      }
      /* Prefer nearest-neighbor when scaling to reduce blur */
      #esp32Feed,
      #captureCanvas {
        image-rendering: pixelated;
        image-rendering: crisp-edges;
      }
    </style>
  </head>
  <body
    style="
      background-color: #000;
      color: #fff;
      font-family: sans-serif;
      overflow: hidden;
      margin: 0;
    "
  >
    <!-- THIS IS THE FIX: Added crossorigin="anonymous" -->
    <img
      id="esp32Feed"
      crossorigin="anonymous"
      alt="Camera Feed"
      style="
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        object-fit: contain; /* was 'cover' (causes resampling blur) */
        z-index: -1;
        opacity: 0.3;
      "
    />

    <canvas id="captureCanvas" style="display: none"></canvas>

    <div
      style="
        display: flex;
        flex-direction: column;
        height: 100vh;
        justify-content: space-between;
        align-items: center;
        padding: 32px;
      "
    >
      <div
        id="status"
        style="
          font-size: 1.5rem;
          font-weight: 500;
          text-align: center;
          height: 80px;
        "
      >
        Initializing system...
      </div>

      <div style="display: flex; justify-content: center; align-items: center">
        <button
          id="listenButton"
          disabled
          style="
            background-color: #2563eb;
            color: white;
            border-radius: 9999px;
            width: 160px;
            height: 160px;
            display: flex;
            justify-content: center;
            align-items: center;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1),
              0 4px 6px -2px rgba(0, 0, 0, 0.05);
            border: none;
            transition: all 0.2s ease;
          "
        >
          <svg
            xmlns="http://www.w3.org/2000/svg"
            id="micIcon"
            style="width: 80px; height: 80px"
            viewBox="0 0 24 24"
            fill="currentColor"
          >
            <path
              d="M12 14c1.66 0 2.99-1.34 2.99-3L15 5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm5.3-3c0 3-2.54 5.1-5.3 5.1S6.7 14 6.7 11H5c0 3.41 2.72 6.23 6 6.72V21h2v-3.28c3.28-.49 6-3.31 6-6.72h-1.7z"
            />
          </svg>
        </button>
      </div>
    </div>

    <script>
      // === CONFIG ===
      const SERVER_BASE_URL = "http://10.248.176.27:8000";
      const SERVER_API_URL = `${SERVER_BASE_URL}/api/analyze`;
      // Route video through our proxy to avoid CORS/tainted canvas
      const PROXIED_STREAM_URL = `${SERVER_BASE_URL}/camera_stream`;
      // WebSocket URL for Vosk voice streaming
      const VOICE_WS_URL = (() => {
        try {
          const u = new URL(SERVER_BASE_URL);
          u.protocol = u.protocol === "https:" ? "wss:" : "ws:";
          u.pathname = u.pathname.replace(/\/$/, "") + "/ws/voice";
          return u.toString();
        } catch (e) {
          return SERVER_BASE_URL.replace(/^http/, "ws") + "/ws/voice";
        }
      })();

      // ... (rest of the script is unchanged) ...
      const esp32Feed = document.getElementById("esp32Feed");
      const canvas = document.getElementById("captureCanvas");
      const listenButton = document.getElementById("listenButton");
      const statusDiv = document.getElementById("status");
      const micIcon = document.getElementById("micIcon");

      let recognition; // kept for fallback, not used when Vosk WS is active
      let isListening = false;
      let isSystemReady = false;
      let alwaysListeningEnabled = false; // hands-free auto-restart for WS

      // Client-side reading state (Web Speech Synthesis)
      let currentUtterance = null;
      let isReading = false;
      let isReadingPaused = false;
      let readChunks = [];
      let readIndex = 0;
      let chunkStartOffset = 0; // resume within chunk from this char offset
      let lastBoundaryChar = 0; // absolute char index within original chunk

      // Audio/WS for server-side Vosk
      let audioCtx = null;
      let mediaStream = null;
      let mediaSource = null;
      let processor = null;
      let voiceWs = null;

      function speak(text) {
        console.log("Speaking:", text);
        // If we're reading something, cancel it before speaking short prompts
        if (isReading && speechSynthesis) {
          try {
            speechSynthesis.cancel();
          } catch (e) {}
          isReading = false;
          isReadingPaused = false;
          currentUtterance = null;
        }
        statusDiv.textContent = text;
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = "en-US";
        speechSynthesis.speak(utterance);
      }

      // --- Reading controls (client-side, chunked) ---
      function splitTextToChunks(text, maxLen = 180) {
        // Split by sentence boundaries first
        const sentences = text
          .replace(/\s+/g, " ")
          .trim()
          .split(/(?<=[.!?])\s+/);
        const chunks = [];
        for (const s of sentences) {
          if (s.length <= maxLen) {
            if (s) chunks.push(s);
          } else {
            // further split long sentences by length
            let start = 0;
            while (start < s.length) {
              chunks.push(s.slice(start, Math.min(start + maxLen, s.length)));
              start += maxLen;
            }
          }
        }
        return chunks;
      }

      function speakCurrentChunk() {
        if (!isReading || isReadingPaused) return;
        if (readIndex >= readChunks.length) {
          // finished
          isReading = false;
          isReadingPaused = false;
          currentUtterance = null;
          readChunks = [];
          readIndex = 0;
          statusDiv.textContent = "Finished reading.";
          return;
        }
        const full = readChunks[readIndex];
        const text = full.slice(chunkStartOffset);
        if (!text || !text.trim()) {
          readIndex++;
          chunkStartOffset = 0;
          lastBoundaryChar = 0;
          speakCurrentChunk();
          return;
        }
        try {
          speechSynthesis.cancel();
        } catch (e) {}
        currentUtterance = new SpeechSynthesisUtterance(text);
        currentUtterance.lang = "en-US";
        currentUtterance.onstart = () => {
          statusDiv.textContent = `Reading (${readIndex + 1}/${
            readChunks.length
          })...`;
        };
        // Track word boundaries so we can resume inside the chunk with minimal rewind
        currentUtterance.onboundary = (e) => {
          if (typeof e.charIndex === "number") {
            lastBoundaryChar = chunkStartOffset + e.charIndex;
          }
        };
        currentUtterance.onend = () => {
          // advance only if not paused/stopped
          if (!isReadingPaused && isReading) {
            readIndex++;
            chunkStartOffset = 0;
            lastBoundaryChar = 0;
            // schedule next chunk
            setTimeout(() => speakCurrentChunk(), 20);
          }
        };
        currentUtterance.onerror = (e) => {
          console.error("Reading error:", e.error || e);
        };
        speechSynthesis.speak(currentUtterance);
      }

      function startClientReading(text) {
        if (!text || !text.trim()) {
          speak("No readable text detected.");
          return;
        }
        try {
          speechSynthesis.cancel();
        } catch (e) {}
        isReading = true;
        isReadingPaused = false;
        readChunks = splitTextToChunks(text);
        readIndex = 0;
        chunkStartOffset = 0;
        lastBoundaryChar = 0;
        if (readChunks.length === 0) {
          speak("No readable text detected.");
          isReading = false;
          return;
        }
        statusDiv.textContent = "Reading text...";
        speakCurrentChunk();
      }

      function pauseClientReading() {
        if (isReading && !isReadingPaused) {
          isReadingPaused = true;
          try {
            speechSynthesis.cancel();
          } catch (e) {}
          // Compute resume point inside the current chunk with a tiny rewind
          // Use lastBoundaryChar if available; else leave chunkStartOffset unchanged
          if (lastBoundaryChar > 0) {
            const REWIND_CHARS = 12; // small rewind for context
            chunkStartOffset = Math.max(0, lastBoundaryChar - REWIND_CHARS);
          }
          statusDiv.textContent = "Paused reading.";
        } else {
          // keep quiet to avoid echo
          statusDiv.textContent = "No active reading to pause.";
        }
      }

      function resumeClientReading() {
        if (isReading && isReadingPaused) {
          isReadingPaused = false;
          statusDiv.textContent = "Resumed reading.";
          // continue from current chunkStartOffset
          speakCurrentChunk();
        } else {
          statusDiv.textContent = "Reading is not paused.";
        }
      }

      function stopClientReading() {
        if (isReading || isReadingPaused) {
          try {
            speechSynthesis.cancel();
          } catch (e) {}
          isReading = false;
          isReadingPaused = false;
          currentUtterance = null;
          readChunks = [];
          readIndex = 0;
          chunkStartOffset = 0;
          lastBoundaryChar = 0;
          statusDiv.textContent = "Stopped reading.";
        } else {
          statusDiv.textContent = "No active reading.";
        }
      }

      function initSpeechRecognition() {
        const SpeechRecognition =
          window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) {
          speak(
            "Error: Speech recognition is not supported in this browser. Please use Chrome on Android."
          );
          return;
        }
        recognition = new SpeechRecognition();
        recognition.continuous = false;
        recognition.interimResults = false;
        recognition.lang = "en-US";
      }

      // ====== Vosk WS mic streaming ======
      function downsampleBuffer(buffer, inputSampleRate, outSampleRate) {
        if (outSampleRate === inputSampleRate) return buffer;
        const ratio = inputSampleRate / outSampleRate;
        const newLen = Math.round(buffer.length / ratio);
        const result = new Float32Array(newLen);
        let offsetResult = 0;
        let offsetBuffer = 0;
        while (offsetResult < result.length) {
          const nextOffsetBuffer = Math.round((offsetResult + 1) * ratio);
          let accum = 0,
            count = 0;
          for (
            let i = offsetBuffer;
            i < nextOffsetBuffer && i < buffer.length;
            i++
          ) {
            accum += buffer[i];
            count++;
          }
          result[offsetResult] = accum / (count || 1);
          offsetResult++;
          offsetBuffer = nextOffsetBuffer;
        }
        return result;
      }

      function floatTo16BitPCM(input) {
        const buffer = new ArrayBuffer(input.length * 2);
        const view = new DataView(buffer);
        let offset = 0;
        for (let i = 0; i < input.length; i++, offset += 2) {
          let s = Math.max(-1, Math.min(1, input[i]));
          view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
        }
        return buffer;
      }

      // Normalize and debounce control commands received from server
      let lastCmdTs = 0;
      const CMD_DEBOUNCE_MS = 600;
      function handleServerCommand(rawCmd) {
        const cmd = String(rawCmd || "")
          .toLowerCase()
          .trim();
        const now = Date.now();
        if (now - lastCmdTs < CMD_DEBOUNCE_MS) return;
        lastCmdTs = now;
        console.log("[VOICE] control:", cmd);

        const isPause =
          /(^|\b)(pause|pause reading|pause the reading)($|\b)/.test(cmd);
        const isResume =
          /(^|\b)(resume|continue|resume reading|continue reading)($|\b)/.test(
            cmd
          );
        const isStop =
          /(^|\b)(stop reading|cancel reading|end reading|stop)($|\b)/.test(
            cmd
          );
        const isRead =
          /(^|\b)(read this|read text|read the text|read)($|\b)/.test(cmd);
        const isWho = /(^|\b)(who|who is|who's)($|\b)/.test(cmd);
        const isWhat = /(^|\b)(what|what is|what's)($|\b)/.test(cmd);

        if (isPause) return void pauseClientReading();
        if (isResume) return void resumeClientReading();
        if (isStop) return void stopClientReading();
        if (isRead) return void sendFrameToServer("read");
        if (isWho) return void sendFrameToServer("who");
        if (isWhat) return void sendFrameToServer("what");
      }

      async function startVoskListening() {
        if (voiceWs && voiceWs.readyState === WebSocket.OPEN) return;
        try {
          mediaStream = await navigator.mediaDevices.getUserMedia({
            audio: {
              channelCount: 1,
              echoCancellation: false,
              noiseSuppression: false,
              autoGainControl: false,
            },
          });
        } catch (e) {
          speak("Microphone permission denied.");
          return;
        }

        audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        mediaSource = audioCtx.createMediaStreamSource(mediaStream);
        processor = audioCtx.createScriptProcessor(4096, 1, 1);
        processor.onaudioprocess = (e) => {
          const input = e.inputBuffer.getChannelData(0);
          const down = downsampleBuffer(input, audioCtx.sampleRate, 16000);
          const pcm = floatTo16BitPCM(down);
          if (voiceWs && voiceWs.readyState === WebSocket.OPEN) {
            voiceWs.send(pcm);
          }
        };
        mediaSource.connect(processor);
        // Some browsers require connecting to destination for processor to run
        try {
          processor.connect(audioCtx.destination);
        } catch (e) {}

        voiceWs = new WebSocket(VOICE_WS_URL);
        voiceWs.binaryType = "arraybuffer";
        voiceWs.onopen = () => {
          isListening = true;
          listenButton.classList.add("listening");
          statusDiv.textContent = "Hands-free listening (Vosk server) enabled.";
        };
        voiceWs.onmessage = (event) => {
          try {
            const msg = JSON.parse(event.data);
            if (msg.type === "control") {
              handleServerCommand(msg.command);
            } else if (msg.type === "error") {
              statusDiv.textContent = `Voice error: ${msg.message}`;
            }
          } catch (e) {
            // ignore non-JSON messages
          }
        };
        voiceWs.onclose = () => {
          isListening = false;
          listenButton.classList.remove("listening");
          if (alwaysListeningEnabled) {
            setTimeout(() => {
              if (alwaysListeningEnabled) startVoskListening();
            }, 700);
          }
        };
        voiceWs.onerror = () => {
          statusDiv.textContent = "Voice WebSocket error.";
        };
      }

      function stopVoskListening() {
        alwaysListeningEnabled = false;
        if (voiceWs) {
          try {
            voiceWs.close();
          } catch (e) {}
          voiceWs = null;
        }
        if (processor) {
          try {
            processor.disconnect();
          } catch (e) {}
          processor = null;
        }
        if (mediaSource) {
          try {
            mediaSource.disconnect();
          } catch (e) {}
          mediaSource = null;
        }
        if (audioCtx) {
          try {
            audioCtx.close();
          } catch (e) {}
          audioCtx = null;
        }
        if (mediaStream) {
          try {
            mediaStream.getTracks().forEach((t) => t.stop());
          } catch (e) {}
          mediaStream = null;
        }
        isListening = false;
        listenButton.classList.remove("listening");
        statusDiv.textContent = "Stopped listening.";
      }

      function initESP32Camera() {
        // Load the camera through the server proxy so the browser sees proper CORS headers
        esp32Feed.src = PROXIED_STREAM_URL;
        let hasLoaded = false;

        esp32Feed.onload = () => {
          if (hasLoaded) return;
          hasLoaded = true;

          setTimeout(() => {
            canvas.width = esp32Feed.naturalWidth || 640;
            canvas.height = esp32Feed.naturalHeight || 480;
            // Keep aspect ratio consistent for display
            esp32Feed.style.aspectRatio = `${canvas.width} / ${canvas.height}`;
            console.log(
              "Camera stream loaded. Canvas size set:",
              canvas.width,
              "x",
              canvas.height
            );

            isSystemReady = true;
            listenButton.disabled = false;
            speak("System ready. Tap to speak.");
          }, 500);
        };

        esp32Feed.onerror = () => {
          console.error("ESP32 Feed error!");
          speak(
            "Error: Could not load camera feed from server. Check server console."
          );
        };
      }

      function toggleListen() {
        if (!isSystemReady) {
          speak("Please wait for the system to initialize.");
          return;
        }

        if (isListening) {
          stopVoskListening();
        } else {
          alwaysListeningEnabled = true; // enable auto-reconnect for WS
          startVoskListening();
        }
      }

      function processCommand(command) {
        const norm = command.toLowerCase();
        // Reading control intents first
        if (/(^|\b)(pause|pause reading)($|\b)/.test(norm)) {
          pauseClientReading();
        } else if (
          /(^|\b)(resume|continue|resume reading|continue reading)($|\b)/.test(
            norm
          )
        ) {
          resumeClientReading();
        } else if (
          /(^|\b)(stop reading|cancel reading|end reading|stop)($|\b)/.test(
            norm
          )
        ) {
          stopClientReading();
        } else if (norm.includes("who")) {
          sendFrameToServer("who");
        } else if (norm.includes("read")) {
          sendFrameToServer("read");
        } else if (norm.includes("what")) {
          sendFrameToServer("what");
        } else {
          speak(`I heard "${command}", but I don't know that command.`);
        }
      }

      async function sendFrameToServer(command) {
        // For read command, avoid speaking the prompt to reduce overlap
        if (command !== "read") {
          speak(`Processing your request to ${command}...`);
        } else {
          statusDiv.textContent = `Processing your request to ${command}...`;
        }
        console.log("LOG: Sending frame to server...");

        const context = canvas.getContext("2d", { willReadFrequently: true });
        context.imageSmoothingEnabled = false;

        let imageData;
        try {
          context.clearRect(0, 0, canvas.width, canvas.height);
          context.drawImage(esp32Feed, 0, 0, canvas.width, canvas.height);
          // Prefer lossless PNG to preserve detail
          imageData = canvas.toDataURL("image/png").split(",")[1];
          // Or, if bandwidth matters:
          // imageData = canvas.toDataURL("image/jpeg", 0.98).split(",")[1];
        } catch (e) {
          console.error("Canvas drawImage error:", e); // This is where it failed last time
          speak(
            "Error capturing frame. Please check browser flags for insecure origins"
          );
          return;
        }

        try {
          const response = await fetch(SERVER_API_URL, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({ command, image_b64: imageData }),
          });

          console.log("LOG: Got response from server");

          if (!response.ok)
            throw new Error(`Server error: ${response.statusText}`);

          const result = await response.json();
          if (command === "read") {
            if (!result.text || !result.text.trim()) {
              speak("I couldn't detect any readable text.");
            } else {
              startClientReading(result.text);
            }
          } else {
            speak(result.text);
          }
        } catch (err) {
          console.error("LOG: Error fetching from server:", err);
          speak(
            "Sorry, I could not connect to the AI server. Please check the network."
          );
        }
      }

      window.onload = () => {
        initSpeechRecognition();
        initESP32Camera();
        listenButton.addEventListener("click", toggleListen);
        speak("Welcome to Assistive AI. Initializing system. Please wait.");
      };
    </script>
  </body>
</html>
